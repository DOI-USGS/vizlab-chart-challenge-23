---
title: "Not your parents' baselines"
author: "Althea Archer"
date: "2023-03-16"
output: html_document
---

## Instructions for using this template

1. Put all comments either in the html text (like this) or in the `r` chunks (delineated with 3 "`" marks) to explain your process to Vizlab reviewers
2. Put all R code in the chunks. They're organized for you to (in order): do some set-up steps (load libraries), load in any data (manually or with built in packages), process your data for plotting, create plot using ggplot, and finally compose the twitter image with cowplot.
3. You can run individual code chunks while developing code and plots. Note: the plots that are created in the RStudio coding window *do not* match the plot as created through the ggsave function. 
4. To preview the final plot, use the `Knit` button above. That will create a preview of the final plot in the output `html` and will save the `png` in the `out/` folder. On many systems, the preview in the script document and the html will not match the output `png`, so please refer to the `png` for final proofing.
5. When you're happy with the final plot, fill out the supporting information at the bottom and push to gitlab for review. Note: only commit input data if it can't be downloaded directly with code (e.g., `sbtools` or `dataRetrieval`).

This website has great information on how to use RMarkdown: https://yihui.org/knitr/

## Set up

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```



Define libraries here. 

```{r libraries, warning=FALSE, message=FALSE}
# Load libraries
library(tidyverse) # includes ggplot
library(readr) # best for reading and writing csvs

library(sbtools)
library(readr)

# These are used for the layout of the viz
library(cowplot) # for laying out the final plot
library(sysfonts) # for text editing
library(showtext) # for adding in google fonts
library(magick) # for adding logo
```



## Load files

Save any files that you are using directly in the `in` folder. Then read them in to the environment with this chunk of code. Remember, use the RStudio project and relative file pathways, and never include any personal computer file pathways.

This is also a good place to load any data from other built-in packages such as `dataRetrieval` or `spData`, etc.


```{r load}
# Load drought properties data from ScienceBase
authenticate_sb()
# 1921-2020
file_in_1921 <- "in/Streamflow_percentiles_national_1921/01031500.csv"
if(!file.exists(file_in_1921)){ # if files don't exist, download
  sbtools::item_file_download(sb_id = "62f463c5d34eacf5397395d9",
                              names = "Streamflow_percentiles_national_1921.zip",
                              destinations = file_in_1921,
                              overwrite_file = F)
}


# 1981-2020
file_in_1981 <- "in/Streamflow_percentiles_national_1981/01031500.csv"
if(!file.exists(file_in_1981)){ # if files don't exist, download
  sbtools::item_file_download(sb_id = "62f475abd34eacf5397395e8",
                              names = "Streamflow_percentiles_national_1921.zip",
                              destinations = file_in_1981,
                              overwrite_file = F)
}

file_in_metadata <- "in/study_watersheds_metadata.csv"
if(!file.exists(file_in_metadata)){ # if files don't exist, download
  sbtools::item_file_download(sb_id = "62793493d34e8d45aa6e3ba9",
                              names = "study_watersheds_metadata.csv",
                              destinations = file_in_metadata,
                              overwrite_file = F)
}
metadata <- readr::read_csv(file_in_metadata, show_col_types = FALSE) |>
  select(StaID, STANAME, DRAIN_SQKM, HUC02, LAT_GAGE, LNG_GAGE, STATE,
         national_1981, national_1951, national_1921)

# Download US State boundaries as sf object
states_sf <- spData::us_states
# add in state names, too
st_crosswalk <- tibble(NAME = state.name) %>%
  bind_cols(tibble(STATE = state.abb)) 
# Merge in CASC info
states_CASC <- states_sf |>
  mutate("CASC" = case_when(NAME %in% c("Minnesota", "Iowa", "Missouri", 
                                       "Wisconsin", "Illinois", "Indiana", 
                                       "Michigan", "Ohio") ~ "Midwest",
                          NAME %in% c("Montana", "Wyoming", "Colorado", "North Dakota", 
                                       "South Dakota", "Nebraska", "Kansas") ~ "North Central",
                          NAME %in% c("Maine", "New Hampshire", "Vermont", "Massachusetts", 
                                       "Connecticut", "Rhode Island",
                                       "New York", "New Jersey", "Pennsylvania", 
                                       "Delaware", "Maryland", "West Virginia", 
                                       "Virginia", "Kentucky", "District of Columbia") ~ "Northeast",
                          NAME %in% c("Washington", "Oregon", "Idaho") ~ "Northwest",
                          NAME %in% c("New Mexico", "Texas", 
                                      "Oklahoma", "Louisiana") ~ "South Central",
                          NAME %in% c("North Carolina", "South Carolina", "Georgia", "Alabama", 
                                       "Mississippi", "Florida", "Tennessee", 
                                       "Arkansas") ~ "Southeast",
                          NAME %in% c("Arizona", "California", 
                                       "Utah", "Nevada") ~ "Southwest",
                          TRUE ~ "not sorted")) |>
  left_join(st_crosswalk)

```

## Get data ready for plotting
  
This next section is for doing any pre-processing steps, data joins, etc, to get your data ready for plotting with ggplot. Remember that most of the time, ggplot prefers data in a "wide" format.


```{r processing}
# Determine what sites are in both datasets using metadata
metadata <- metadata |>
  mutate(in_both_L5181 = case_when(national_1981 & national_1951 ~ TRUE,
                               TRUE ~ FALSE),
         in_both_L = case_when(national_1981 & national_1921 ~ TRUE,
                               TRUE ~ FALSE))

# Determine focal sites
focal_sites <- metadata$StaID[metadata$in_both_L]
focal_sites5181 <- metadata$StaID[metadata$in_both_L5181]

# Read in all focal sites and rowbind together for each timeframe
## Because I'm using site-level thresholds, only need first row for each site
files_1921 <- sprintf("in/Streamflow_percentiles_national_1921/%s.csv", focal_sites)
t1921_data <- purrr::map(files_1921[1], 
                         ~ readr::read_csv(files_1921, 
                                           n_max = 1, 
                                           show_col_types = FALSE,
                                           col_types = list("StaID" = col_character()))) |>
  reduce(rbind) |>
  # only keep fields we need
  select(StaID, thresh_20_site) |>
  # and rename based on timeframe
  rename(thresh_1921 = thresh_20_site) 

files_1951 <- sprintf("in/Streamflow_percentiles_national_1951/%s.csv", focal_sites5181)
t1951_data <- purrr::map(files_1951[1], 
                         ~ readr::read_csv(files_1951, 
                                           n_max = 1, 
                                           show_col_types = FALSE,
                                           col_types = list("StaID" = col_character()))) |>
  reduce(rbind) |>
  # only keep fields we need
  select(StaID, thresh_20_site) |>
  # and rename based on timeframe
  rename(thresh_1951 = thresh_20_site) 

files_1981 <- sprintf("in/Streamflow_percentiles_national_1981/%s.csv", focal_sites5181)
t1981_data <- purrr::map(files_1981[1], 
                         ~ readr::read_csv(files_1981, 
                                           n_max = 1, 
                                           show_col_types = FALSE,
                                           col_types = list("StaID" = col_character()))) |>
  reduce(rbind) |>
  # only keep fields we need
  select(StaID, thresh_20_site) |>
  # and rename based on timeframe
  rename(thresh_1981 = thresh_20_site) 

# Merge data together (left join)
thresh_data <- t1921_data |> left_join(t1981_data) |> left_join(t1951_data) |>
  # and calculate how much threshold has changed (magnitude, absolute value, and sign)
  mutate(magnitude_change = thresh_1981 - thresh_1921,
         absolute_change = abs(magnitude_change),
         change_text = case_when(magnitude_change > 0 ~ "increase",
                                 magnitude_change < 0 ~ "decrease",
                                 magnitude_change == 0 ~ "no change",
                                 TRUE ~ "no data"))

thresh_data5181 <- t1951_data |> left_join(t1981_data) |>
  # and calculate how much threshold has changed (magnitude, absolute value, and sign)
  mutate(magnitude_change = thresh_1981 - thresh_1951,
         absolute_change = abs(magnitude_change),
         change_text = case_when(magnitude_change > 0 ~ "increase",
                                 magnitude_change < 0 ~ "decrease",
                                 magnitude_change == 0 ~ "no change",
                                 TRUE ~ "no data"))


# Add in metadata for stations
all_data <- thresh_data |> 
  left_join(metadata) |>
  # add in Climate area 
  mutate("CASC" = case_when(STATE %in% c("MN", "IA", "MO", "WI", "IL", "IN", "MI", "OH") ~ "Midwest",
                          STATE %in% c("MT", "WY", "CO", "ND", "SD", "NE", "KS") ~ "North Central",
                          STATE %in% c("ME", "NH", "VT", "MA", "CT", "RI", "DC",
                                       "NY", "NJ", "PA", "DE", "MD", "WV", "VA", "KY") ~ "Northeast",
                          STATE %in% c("WA", "OR", "ID") ~ "Northwest",
                          STATE %in% c("NM", "TX", "OK", "LA") ~ "South Central",
                          STATE %in% c("NC", "SC", "GA", "AL", "MS", "FL", "TN", "AR") ~ "Southeast",
                          STATE %in% c("AZ", "CA", "UT", "NV") ~ "Southwest",
                          TRUE ~ "not sorted"))

# Summarize by HUC2
huc02_summary <- all_data |>
  group_by(HUC02, change_text) |>
  summarise(n = n()) |>
  mutate(proportion = n / sum(n))

# Summarize by State
state_summary <- all_data |>
  group_by(STATE, CASC, change_text) |>
  summarise(n = n()) |>
  mutate(proportionState = n / sum(n)) |>
  left_join(st_crosswalk)




# Summarize by CASC
CASC_summary <- all_data |>
  group_by(CASC, change_text) |>
  summarise(n = n()) |>
  mutate(proportionCASC = n / sum(n)) 

mapping_data <- states_CASC |>
  left_join(CASC_summary |> filter(change_text == "decrease")) 
mapping_data <- mapping_data |> 
  left_join(state_summary |> filter(change_text == "decrease"), by = "STATE")
ggplot(data = mapping_data, aes(group = STATE))+
  geom_sf(aes(fill = proportionState))

wyoming_example_all <- readr::read_csv("in/Streamflow_percentiles_national_1921/13011000.csv") 
wyoming_example21 <- readr::read_csv("in/Streamflow_percentiles_national_1921/13011000.csv") |>
  filter(year == 1985)
wyoming_example51 <- readr::read_csv("in/Streamflow_percentiles_national_1951/13011000.csv") |>
  filter(year == 1985)
wyoming_example81 <- readr::read_csv("in/Streamflow_percentiles_national_1981/13011000.csv") |>
  filter(year == 1985)
ggplot(data = wyoming_example, aes(x = jd, y = value)) + geom_line(aes(color = year), alpha = 0.2) +
  geom_line(data = wyoming_example21, aes(y = thresh_20_jd), color = "pink") +
  geom_line(data = wyoming_example51, aes(y = thresh_20_jd), color = "purple") + 
  geom_line(data = wyoming_example81, aes(y = thresh_20_jd), color = "maroon") +
  coord_polar() +
  ylim(c(0,3000))

median(wyoming_example$value)
median(wyoming_example$value[wyoming_example$year > 1980])

```

## Set up main plot

This chunk is where the main ggplot grob definition and set-up occurs. You may have several ggplot grobs, depending on your approach. For each one, if you define it with the `<-` assignment operator and surround the whole code statement in parentheses, you'll be able to preview here what the plot looks like before the next cowplot step.

> Note: The hardest part of this process is getting text to be the optimum size in the final `png` for Twitter. The font sizes here will **not** be the same as in the final image after the cowplot step. Make sure to check the output `png` for true font sizing and plot composition. 

```{r plotting}
# Load some custom fonts and set some custom settings
font_legend <- "Pirata One"
sysfonts::font_add_google("Pirata One")
supporting_font <- "Source Sans Pro"
sysfonts::font_add_google("Source Sans Pro")
showtext::showtext_opts(dpi = 300, regular.wt = 200, bold.wt = 700)
showtext::showtext_auto(enable = TRUE)

# Define colors
background_color = "#0A1927"
font_color = "#ffffff"

# The background canvas for your viz
canvas <- grid::rectGrob(
  x = 0, y = 0, 
  width = 16, height = 9,
  gp = grid::gpar(fill = background_color, alpha = 1, col = background_color)
)

# Load in USGS logo (also a black logo available)
usgs_logo <- magick::image_read("../usgs_logo_white.png") 

# Main plot
(main_plot <- ggplot()
 )



```


## Produce final plot

Here, use `cowplot` and `ggsave` to create the final viz for sharing out on Twitter. This template includes adding the USGS logo, title, text, etc.

**Make sure to use the format for saving your png with the date of the prompt that you've been assigned to!** (e.g., `20230401_part-to-whole_cnell.png`)

```{r cowplot, fig.width = 16, fig.height = 9}
ggdraw(ylim = c(0,1), # 0-1 scale makes it easy to place viz items on canvas
       xlim = c(0,1)) +
  # a background
  draw_grob(canvas,
            x = 0, y = 1,
            height = 9, width = 16,
            hjust = 0, vjust = 1) +
  # the main plot
  draw_plot(main_plot,
            x = 0.01,
            y = 0.01,
            height = 1) +
  # explainer text
  draw_label("Explainer text",
             fontfamily = supporting_font,
             x = 0.96,   
             y = 0.05,
             size = 14,
             hjust = 1,
             vjust = 0,
             color = font_color)+
  # Title
  draw_label("Title",
             x = 0.04,
             y = 0.285,
             hjust = 0,
             vjust = 1,
             lineheight = 0.75,
             fontfamily = font_legend,
             color = font_color,
             size = 55) +
  # Add logo
  draw_image(usgs_logo, 
             x = 0.04,
             y = 0.05,
             width = 0.1, 
             hjust = 0, vjust = 0, 
             halign = 0, valign = 0)

# Save the final image in Twitter's 16 by 9 format
# !! Use format for saving with the date of your prompt: 
#         YYYYMMDD_prompt_name ()
# e.g. `20230101_part-to-whole-cnell.png`
ggsave(filename = "out/20230000_prompt_name.png", 
       width = 16, height = 9, dpi = 300)
```

## Supporting information

### Key takeaways of this viz (1-2 sentences each)

1. Key takeaways here

### Data source(s)

Data sources here.

