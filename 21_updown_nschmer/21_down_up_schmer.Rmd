---
title: "Chart Challenge day 21: Downwards/ upwards"
author: "Natalie Schmer"
date: "2023-04-13"
output: html_document
editor_options: 
  chunk_output_type: inline
---

## Instructions for using this template

> Make sure you have copied the template subdirectory and named it with your prompt's date, prompt's name and your name (e.g., `/01_part-to-whole_cnell`) before editing this document!

1. Put all comments either in the html text (like this) or in the `r` chunks (delineated with 3 "`" marks) to explain your process to Vizlab reviewers
2. Put all R code in the chunks. They're organized for you to (in order): do some set-up steps (load libraries), load in any data (manually or with built in packages), process your data for plotting, create plot using ggplot, and finally compose the twitter image with cowplot.
3. You can run individual code chunks while developing code and plots. Note: the plots that are created in the RStudio coding window *do not* match the plot as created through the ggsave function. 
4. To preview the final plot, use the `Knit` button above. That will create a preview of the final plot in the output `html` and will save the `png` in the `out/` folder. On many systems, the preview in the script document and the html will not match the output `png`, so please refer to the `png` for final proofing.
5. When you're happy with the final plot, fill out the supporting information at the bottom and push to gitlab for review. Note: only commit input data if it can't be downloaded directly with code (e.g., `sbtools` or `dataRetrieval`).

This website has great information on how to use RMarkdown: https://yihui.org/knitr/

## Set up

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



Define libraries here. 

```{r libraries, warning=FALSE, message=FALSE}
# Load libraries
library(tidyverse) # includes ggplot
library(readr) # best for reading and writing csvs

# These are used for the layout of the viz
library(cowplot) # for laying out the final plot
library(sysfonts) # for text editing
library(showtext) # for adding in google fonts
library(magick) # for adding logo

library(dataRetrieval)
library(directlabels)
library(here)
library(lubridate)
library(tidylog)
library(gghighlight)
#install.packages("geofacet")
library(geofacet)
```



## Load files

Save any files that you are using directly in the `in` folder. Then read them in to the environment with this chunk of code. Remember, use the RStudio project and relative file pathways, and never include any personal computer file pathways.

This is also a good place to load any data from other built-in packages such as `dataRetrieval` or `spData`, etc.


```{r load}
# Data files:

temp_data_avg <- readRDS(file = here::here("21_updown_nschmer/in/temp_data_avg.RDS"))
final_list <- readRDS(here::here("21_updown_nschmer/in/final_sites.RDS"))

```

## Get data ready for plotting
  
This next section is for doing any pre-processing steps, data joins, etc, to get your data ready for plotting with ggplot. Remember that most of the time, ggplot prefers data in a "wide" format.

```{r, eval=FALSE}

# Set up states  
stateCd

states_og <- stateCd[c(2:9, 11:52),]  
states <- c(states_og$STUSAB)

# Steps
#1. pull sites for all states  
#2. pull all sites dv for yesterday
#3. remove any that have "P Dis" (Discontinued)
#4. choose from remaining sites
#NEED TO COMPARE in each state: Sites with earliest begin date vs sites with highest count 


sites_all <- data.frame()

manual_sites <- data.frame()

#i <- "CA"

for(i in unique(states)){
  
  message("Getting: ", i)
  
   temps_sites <- whatNWISdata(stateCd = i, service = "dv", parameterCd = "00010", statCd= "00003")

  # want only streams 
  temps_sites <- temps_sites %>% 
    filter(site_tp_cd == "ST")
   
  #some sites are discontinued, pull to see and then delete those sites 
  site_test <- c(temps_sites$site_no)
  
  disc.test <- readNWISdv(site_test, "00010", "2022-04-15", "2023-04-15")
  
  disc.test <- renameNWISColumns(disc.test)
  
  #remove sites that have any discontinued
  discontinued <- disc.test %>% 
    filter(Wtemp_cd == "P Dis")
  
  # remove those stations from the station list and then:
  # 1. calculate days between beginning and today
  # 2. calculate data percentage (want at least 75)
  # 3. Take 5 oldest
  
  temps_sites <- temps_sites %>% 
    filter(!site_no %in% discontinued$site_no) 
  

  temps_sites_2 <- temps_sites %>% 
    arrange(desc(end_date)) %>% 
    slice_max(end_date) %>%
    mutate(state =i,
           n_days = as.numeric(difftime(end_date, begin_date, units = "days")),
           dat_pct = (count_nu/ n_days)*100) %>% 
      filter(dat_pct > 75) %>%
    slice_min(begin_date, n = 5, with_ties = F)

  
  if(nrow(temps_sites_2) < 5){
 
     temps_sites_2 <- temps_sites %>% 
       arrange(desc(end_date)) %>%
       mutate(state =i,
              n_days = as.numeric(difftime(end_date, begin_date, units = "days")),
              dat_pct = (count_nu/ n_days)*100) %>% 
       slice_max(count_nu, n = 5, with_ties = F)
    
    sites_all <- bind_rows(sites_all, temps_sites_2)

           } else
  
  sites_all <- bind_rows(sites_all, temps_sites_2)
  
}

# Verification 
states_count <- sites_all %>% 
  group_by(state) %>% 
  mutate(site_count = n())

states_repull <- states_count %>% 
  filter(site_count < 5)


unique(states_repull$state)

# Come on, Vermont..... 
   temps_sites <- whatNWISdata(stateCd = "VT", service = "dv", parameterCd = "00010", statCd= "00003")

  # want only streams 
  temps_sites <- temps_sites %>% 
    filter(site_tp_cd == "ST")
   
  # Looks like there's literally only 3 stream sites?!? 


# write this to "in" 
#saveRDS(sites_comparison, file = here::here("21_updown_nschmer/in/sites_comparison.RDS"))


# save 

saveRDS(sites_all, file = here::here("21_updown_nschmer/in/final_sites.RDS"))


final_list <- readRDS(here::here("21_updown_nschmer/in/final_sites.RDS"))

sites <- c(final_list$site_no)


temp_data <- readNWISdv(sites, "00010", "", "", "00003")

temp_data <- renameNWISColumns(temp_data)

saveRDS(temp_data, file = here::here("21_updown_nschmer/in/temp_data.RDS"))
names(temp_data)

```

Pull and format data 
```{r, eval = F}

final_list <- readRDS(here::here("21_updown_nschmer/in/final_sites.RDS"))

temp_data <- readRDS(here::here("21_updown_nschmer/in/temp_data.RDS"))

#not all columns are labeled as wtemp so need to pivot 

temp_data_avg <- temp_data %>% 
  mutate(DOY = yday(Date)) %>% 
  relocate(DOY, .after = "Date") %>% 
  select(-contains("cd")) %>% 
  pivot_longer(4:ncol(.),
               names_to = "og_col",
               values_to = "temp_value") %>% 
  group_by(DOY, site_no) %>% 
  mutate(mean = mean(temp_value, na.rm =T)) %>% 
  ungroup() %>% 
  select(DOY, site_no, mean, temp_value) %>% 
  distinct() %>% 
  left_join(., final_list) %>% 
  mutate(start_yr = year(begin_date),
         end_yr = year(end_date),
         Years = end_yr - start_yr
         # ,
         # site_type = case_when(site_tp_cd == "ST" ~ "Stream",
         #                       site_tp_cd == "GW" ~ "Groundwater",
         #                       site_tp_cd == "LK" ~ "Lake",
         #                       site_tp_cd == "ES" ~ "Estuary")
         ) %>% 
  distinct() %>% 
#%>% 
 #  mutate(label = paste0(state, ", ", start_yr, ", ",site_type),
 #         label2 = paste0(start_yr, ", ",site_type)) %>% 
 #  mutate(Region = case_when(state %in% c("WA", "OR", "ID", "AK") ~ "Northwest",
 #                            state %in% c("HI") ~ "Hawaii",
 #                            #state_abrv %in% c("CA") ~ "California",
 #                            state %in% c("NV", "UT", "AZ", "CA") ~ "Southwest",
 #                            state %in% c("MT", "WY", "CO", "SD", "ND", "NE", "KS") ~ "North Central",
 #                            state %in% c("NM", "OK", "LA", "TX") ~ "South Central",
 #                            state %in% c("WI", "MN", "MI", "IL", "MO", "IA", "OH", "IN") ~ "Midwest",
 #                            state %in% c("AR", "TN", "NC", "SC", "GA", "FL", "MS", "AL") ~ "Southeast",
 #                            state %in% c("ME", "CT", "VT", "NH", "DE", "MD", "PA", "VA", "WV", "KY", "RI", "NJ", "NY", "MA") ~ "Northeast")) %>% 
 # #mutate(Region = factor(levels = "Northwest", "North Central", "Midwest", "Northeast", "Hawaii", "Southwest", "South Central", "Southeast")) %>% 
   group_by(state, DOY) %>% 
  mutate(state_avg = mean(temp_value, na.rm =T))
 #   mutate(Temp_Diff = abs(max(mean) - min(mean)))

temp_data_avg <- temp_data_avg %>% 
  mutate(Region = factor(Region, levels = c("Northwest", "North Central", "Midwest", "Northeast", "Hawaii", "Southwest", "South Central", "Southeast"))) 

saveRDS(temp_data_avg, file = here::here("21_updown_nschmer/in/temp_data_avg.RDS"))
```



## Set up main plot

This chunk is where the main ggplot grob definition and set-up occurs. You may have several ggplot grobs, depending on your approach. For each one, if you define it with the `<-` assignment operator and surround the whole code statement in parentheses, you'll be able to preview here what the plot looks like before the next cowplot step.

> Note: The hardest part of this process is getting text to be the optimum size in the final `png` for Twitter. The font sizes here will **not** be the same as in the final image after the cowplot step. Make sure to check the output `png` for true font sizing and plot composition. 

```{r plotting}
# Load some custom fonts and set some custom settings
font_legend <- "Pirata One"
sysfonts::font_add_google("Pirata One")
supporting_font <- "Source Sans Pro"
sysfonts::font_add_google("Source Sans Pro")
showtext::showtext_opts(dpi = 300, regular.wt = 200, bold.wt = 700)
showtext::showtext_auto(enable = TRUE)

# Define colors
background_color = "#0A1927"
font_color = "#ffffff"

# The background canvas for your viz
canvas <- grid::rectGrob(
  x = 0, y = 0, 
  width = 16, height = 9,
  gp = grid::gpar(fill = background_color, alpha = 1, col = background_color)
)

# Load in USGS logo (also a black logo available)
usgs_logo <- magick::image_read(here::here("usgs_logo_white.png")) 

# Main plot
# Remove DC from grid
my_grid <- us_state_grid1 %>% 
  filter(code != "DC")

(main_plot <-  ggplot(temp_data_avg, aes(x = DOY, y = mean, group = site_no)) +
  geom_line()+
    geom_line(aes(x = DOY, y = state_avg, group = site_no, color = state_avg) )+
  facet_geo(~ state, grid = my_grid)+
  ggthemes::theme_few()+
   labs(y = "Mean Temperature for DOY",
        color= "Temperature (C)"))


# ggplot(temp_data_avg %>% filter(state == "MS"), aes(x = DOY, y = mean, group = site_no)) +
#   geom_line()



```

## ALL
```{r, eval=FALSE}
ggplot(temp_data_avg, aes(x = DOY, y = mean, group = state, color = Years)) +
  geom_line()+
  ggthemes::theme_few()+
  #theme_void()+
  #geom_dl(aes(label= label2), method="last.points")+
  #geom_dl(aes(label = state_abrv), method = "first.points")+
  labs(y= "Mean Water Temperature (C)")
```


## Region
```{r, eval=FALSE}
(temp_lines_static_reg <- ggplot(temp_data_avg, aes(x = DOY, y = mean, group = state, color = Years)) +
    geom_line()+
    ggthemes::theme_few()+
    #theme_void()+
    geom_dl(aes(label= label2), method="last.points")+
    geom_dl(aes(label = state), method = "first.points")+
    labs(y= "Mean Water Temperature (C)")+
    facet_wrap(~Region, ncol = 4))
```

## NOT streams
How to retain facets? 
```{r, eval=FALSE}
ggplot(temp_data_avg, aes(x = DOY, y = mean, group = state, color = Years)) +
    geom_line()+
    ggthemes::theme_few()+
    #theme_void()+
    #geom_dl(aes(label= label2), method="last.points")+
    #geom_dl(aes(label = state), method = "first.points")+
    gghighlight(site_type != "Stream")+
    labs(y= "Mean Water Temperature (C)")+
    facet_wrap(~Region, ncol = 4)
```

# Show temp diff
```{r, eval=FALSE}

ggplot(temp_data_avg, aes(x = DOY, y = mean, group = state, color = Temp_Diff)) +
  geom_line()+
  ggthemes::theme_few()+
  #theme_void()+
  #geom_dl(aes(label= label2), method="last.points")+
  #geom_dl(aes(label = state_abrv), method = "first.points")+
  labs(y= "Mean Water Temperature (C)")+
  scale_color_gradient(low = "blue", high = "red")+
  facet_wrap(~Region, ncol = 4)



# check temp dist
hist(unique(temp_data_avg$Temp_Diff))

temp_low_change <- temp_data_avg %>% 
  filter(Temp_Diff < 11)

ggplot(temp_data_avg, aes(x = DOY, y = mean, group = state, color = Temp_Diff)) +
  geom_line()+
  ggthemes::theme_few()+
  #theme_void()+
  #geom_dl(aes(label= label2), method="last.points")+
  #geom_dl(aes(label = state_abrv), method = "first.points")+
  labs(y= "Mean Water Temperature (C)")+
  scale_color_gradient(low = "blue", high = "red")+
  facet_wrap(~Region, ncol = 4)+
  gghighlight(Temp_Diff < 11, keep_scales = T)

```

# "Special" Sites
```{r, eval=FALSE}

ggplot(temp_data_avg, aes(x = DOY, y = mean, group = state, color = Temp_Diff)) +
  geom_line()+
  ggthemes::theme_few()+
  #theme_void()+
  #geom_dl(aes(label= label2), method="last.points")+
  #geom_dl(aes(label = state_abrv), method = "first.points")+
  labs(y= "Mean Water Temperature (C)")+
  scale_color_gradient(low = "blue", high = "red")+
  facet_wrap(~Region, ncol = 4)+
  gghighlight(state %in% c("HI", "AZ", "NM"), keep_scales = T)

```


# Geofaceting (also main)
```{r, eval=F}
ggplot(temp_data_avg, aes(x = DOY, y = mean, group = state)) +
  geom_line()+
  #coord_flip() +
  facet_geo(~ state)+
  ggthemes::theme_few()

```

# Site checking for TX and VT
```{r, eval=FALSE}
# tx <- temp_data_avg %>% 
#   filter(state == "TX")
# 
# tx_cont <- temp_data %>% 
#   filter(site_no == "08086400")

  temps_sites <- whatNWISdata(stateCd = "TX", service = "dv", parameterCd = "00010", statCd= "00003")

  temps_sites <- temps_sites %>% 
    filter(count_nu > 364)

  #some sites are discontinued, pull to see and then delete those sites 
  site_test <- c(temps_sites$site_no)
  
  disc.test <- readNWISdv(site_test, "00010", "2022-04-12", "2023-04-12")
  
  disc.test <- renameNWISColumns(disc.test)
  
  #remove sites that have any discontinued
  discontinued <- disc.test %>% 
    filter(Wtemp_cd == "P Dis")
  
  # remove those stations from the station list and then figure out which site had the max count vs earliest start 
  
  temps_sites <- temps_sites %>% 
    filter(!site_no %in% discontinued$site_no) 

  max_count <- temps_sites %>% 
    filter(end_date == max(end_date)) %>% 
    filter(count_nu == max(count_nu, na.rm =T))
  
  earliest_start <- temps_sites %>% 
    filter(end_date == max(end_date)) %>%
    filter(begin_date == min(begin_date))
  
  site_compare <- rbind(max_count, earliest_start)
  
  site_compare <- site_compare %>% 
    mutate(state = i) %>% 
    relocate(state, everything())
  
  sites_comparison <- bind_rows(sites_comparison, site_compare)

```



## Produce final plot

Here, use `cowplot` and `ggsave` to create the final viz for sharing out on Twitter. This template includes adding the USGS logo, title, text, etc.

**Make sure to use the format for saving your png with the date of the prompt that you've been assigned to!** (e.g., `20230401_part-to-whole_cnell.png`)

```{r cowplot, fig.width = 16, fig.height = 9}
ggdraw(ylim = c(0,1), # 0-1 scale makes it easy to place viz items on canvas
       xlim = c(0,1)) +
  # a background
  draw_grob(canvas,
            x = 0, y = 1,
            height = 9, width = 16,
            hjust = 0, vjust = 1) +
  # the main plot
  draw_plot(main_plot,
            x = 0.01,
            y = 0.01,
            height = 1) +
  # explainer text
  draw_label("Stream temperatures around the US using the 5 longest daily temperature records per state where possible.",
             fontfamily = supporting_font,
             x = 0.96,   
             y = 0.05,
             size = 14,
             hjust = 1,
             vjust = 0,
             color = font_color)+
  # Title
  draw_label("Continental Stream Temperatures",
             x = 0.04,
             y = 0.285,
             hjust = 0,
             vjust = 1,
             lineheight = 0.75,
             fontfamily = font_legend,
             color = font_color,
             size = 55) +
  # Add logo
  draw_image(usgs_logo, 
             x = 0.04,
             y = 0.05,
             width = 0.1, 
             hjust = 0, vjust = 0, 
             halign = 0, valign = 0)

# Save the final image in Twitter's 16 by 9 format
# !! Use format for saving with the date of your prompt: 
#         YYYYMMDD_prompt_name ()
# e.g. `20230101_part-to-whole-cnell.png`
ggsave(filename = "21_updown_nschmer/out/20230421_downupward_nschmer.png", 
       width = 16, height = 9, dpi = 300)
```

## Supporting information

### Key takeaways of this viz (1-2 sentences each)

1. The USGS has a long data collection history, including water temperature 

### Data source(s)

Data sources here.

