---
title: "Example for Chart Challenge"
author: "Althea Archer"
date: "2023-03-10"
output: html_document
---

## Instructions for using this template

> Make sure you have copied the template subdirectory and named it with your prompt's date, prompt's name and your name (e.g., `/01_part-to-whole_cnell`) before editing this document!

1. Put all comments either in the html text (like this) or in the `r` chunks (delineated with 3 "`" marks) to explain your process to Vizlab reviewers
2. Put all R code in the chunks. They're organized for you to (in order): do some set-up steps (load libraries), load in any data (manually or with built in packages), process your data for plotting, create plot using ggplot, and finally compose the twitter image with cowplot.
3. You can run individual code chunks while developing code and plots. Note: the plots that are created in the RStudio coding window *do not* match the plot as created through the ggsave function. 
4. To preview the final plot, use the `Knit` button above. That will create a preview of the final plot in the output `html` and will save the `png` in the `out/` folder. On many systems, the preview in the script document and the html will not match the output `png`, so please refer to the `png` for final proofing.
5. When you're happy with the final plot, fill out the supporting information at the bottom and push to gitlab for review. Note: only commit input data if it can't be downloaded directly with code (e.g., `sbtools` or `dataRetrieval`).

This website has great information on how to use RMarkdown: https://yihui.org/knitr/

## Set up

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```



Define libraries here. 

```{r libraries, warning=FALSE, message=FALSE}
# Load libraries
library(tidyverse) # includes ggplot
library(readr) # best for reading and writing csvs

library(sf)
library(terra)
library(spData)
library(scico)

# These are used for the layout of the viz
library(cowplot) # for laying out the final plot
library(sysfonts) # for text editing
library(showtext) # for adding in google fonts
library(magick) # for adding logo
```



## Load files

Save any files that you are using directly in the `in` folder. Then read them in to the environment with this chunk of code. Remember, use the RStudio project and relative file pathways, and never include any personal computer file pathways.

This is also a good place to load any data from other built-in packages such as `dataRetrieval` or `spData`, etc.


```{r load}
#### Define global variables
focal_month <- "February"
focal_month_numeric <- 2
focal_month_label <- ifelse(focal_month_numeric < 10, 
                            sprintf("0%s", focal_month_numeric),
                            focal_month_numeric)
focal_year <- 2023

# This is a look-up table for the day-of-year of the first of the month, based on year
date_code_lookup <- data.frame(
  common_yr = c("001", "032", "060", "091", "121", "152", 
                 "182", "213", "244", "274", "305", "335"),
  leap_yr = c("001", "032", "061", "092", "122", "153", 
               "183", "214", "245", "275", "306", "336")
)

# Function to determine the correct code given year and month
code_given_year_mo <- function(month_numeric, year){
  if(year %% 4 == 0){ #if leap year
    return(date_code_lookup$leap_yr[month_numeric])
  } else { # otherwise
    return(date_code_lookup$common_yr[month_numeric])
  }
}

# Return date code for *this* year and month
focal_date_code <- code_given_year_mo(month_numeric = focal_month_numeric,
                                      year = focal_year)

# Read in the snow cover index (SCI) raster
snow_cover_files <- list.files(path = "in", 
                               pattern = glob2rx(sprintf("^MOD10CM.A%s%s*tiff", focal_year, focal_date_code)))
snow_cover <- terra::rast(paste0("in/", snow_cover_files))

# Read in past data
number_years_past <- 20 # change if doing longer than 5 years
past_snow_cover_files <- NULL # empty, to add list of files to

for(yy in 1:number_years_past){ # for each year in the past
  
  # Temporary year, code, and file name
  temp_year <- focal_year - yy
  temp_code <- code_given_year_mo(month_numeric = focal_month_numeric, year = temp_year)
  
  temp_file <- snow_cover_files <- list.files(path = "in", 
                                              pattern = glob2rx(sprintf("^MOD10CM.A%s%s*tiff", temp_year, temp_code)))

  # Append file name to list
  past_snow_cover_files <- c(past_snow_cover_files, temp_file)
} 

# Create raster stack of all past years' rasters
past_snow_cover <- terra::rast(paste0("in/", past_snow_cover_files))


# Download US State boundaries as sf object
states_shp <- spData::us_states



```

## Get data ready for plotting
  
This next section is for doing any pre-processing steps, data joins, etc, to get your data ready for plotting with ggplot. Remember that most of the time, ggplot prefers data in a "wide" format.


```{r processing}
# Reclassify values 211, 250, 253, 254, and 255 to NA
snow_cover_reclass <- terra::classify(snow_cover, cbind(c(211, 250, 253, 254, 255), NA))
past_snow_cover_reclass <- terra::classify(past_snow_cover, cbind(c(211, 250, 253, 254, 255), NA))

# Average over past years
mean_past_snow_cover <- terra::mean(past_snow_cover_reclass, na.rm = TRUE)

# Reproject the sf object to match the projection of the raster
states_rast_proj <- states_shp |> sf::st_transform(terra::crs(snow_cover_reclass))

# Clip to state boundaries (may have to remove "vect()")
snow_cover_clip <- terra::crop(x = snow_cover_reclass, y = terra::vect(states_rast_proj), mask = TRUE)
past_snow_cover_clip <- terra::crop(x = mean_past_snow_cover, y = terra::vect(states_rast_proj), mask = TRUE)

# USGS EA Conic
crs_USGS <- "+proj=aea +lat_0=23 +lon_0=-96 +lat_1=29.5 +lat_2=45.5 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs"

# Reproject to match USGS EA Conic for plotting
snow_cover_clip_project <- terra::project(snow_cover_clip, crs_USGS)
past_snow_cover_clip_project <- terra::project(past_snow_cover_clip, crs_USGS)

# Reproject the state boundary sf object to match the projection of the raster
states_proj <- states_shp |> sf::st_transform(crs_USGS)

# make hex tesselation of CONUS
columns <- 50
rows <- 50
hex_grid <- states_proj |> 
  # using the project states boundaries, make a hexagon grid that is 70 by 70 across the US
  sf::st_make_grid(n = c(columns, rows), 
               what = "polygons", 
               # if square = TRUE, then square. Otherwise hexagonal
               square = FALSE) |>
  sf::st_as_sf() |>
  mutate(geometry = x) |>
  mutate(hex = as.character(row_number()))


# Extract values to the hexagon grid from the masked raster
extract_snow_cover_hex <- terra::extract(x = snow_cover_clip_project, vect(hex_grid))
colnames(extract_snow_cover_hex) <- c("ID", "mean")

past_extract_snow_cover_hex <- terra::extract(x = past_snow_cover_clip_project, vect(hex_grid))
colnames(past_extract_snow_cover_hex) <- c("ID", "mean")

# Calculate mean snow cover by hex
means_by_hex <- extract_snow_cover_hex |> 
  group_by(ID) |>
  summarise(meanSCI = mean(mean, na.rm = T))

past_means_by_hex <- past_extract_snow_cover_hex |>
  group_by(ID) |>
  summarise(meanPastSCI = mean(mean, na.rm = T))


# Calculate SCI means and left-join to the hexagon grid sf object
SCI_hex_grid <- hex_grid |>
  mutate(ID = as.numeric(hex)) |>
  left_join(means_by_hex) |>
  left_join(past_means_by_hex) |>
  # Calculate means and use "cut" to categorize them
  mutate(compare_means = meanSCI - meanPastSCI,
         category = cut(compare_means, 
                        breaks = c(-Inf, -75, -50, -25, -1, 1, 25, 50, 75, Inf ),
                        labels = c("-75%", 
                                   "-50 to -75%", 
                                   "-25 to -50%", 
                                   "  -1 to -25%", 
                                   "No change", 
                                   "  +1 to 25%", 
                                   "+25 to 50%", 
                                   "+50 to 75%",
                                   "+75%")),
         percent_of_normal = (round(meanSCI,0)/round(meanPastSCI,0))*100,
         diff_div_normal = ((meanSCI - meanPastSCI)/meanPastSCI)*100) |>
  filter(! is.na(meanSCI)) # delete hexagons outside the US boundaries



```

## Set up main plot

This chunk is where the main ggplot grob definition and set-up occurs. You may have several ggplot grobs, depending on your approach. For each one, if you define it with the `<-` assignment operator and surround the whole code statement in parentheses, you'll be able to preview here what the plot looks like before the next cowplot step.

> Note: The hardest part of this process is getting text to be the optimum size in the final `png` for Twitter. The font sizes here will **not** be the same as in the final image after the cowplot step. Make sure to check the output `png` for true font sizing and plot composition. 

```{r plotting}
# Load some custom fonts and set some custom settings
font_legend <- "Pirata One"
sysfonts::font_add_google("Pirata One")
supporting_font <- "Source Sans Pro"
sysfonts::font_add_google("Source Sans Pro")
showtext::showtext_opts(dpi = 300, regular.wt = 200, bold.wt = 700)
showtext::showtext_auto(enable = TRUE)

# Define colors
background_color = "white"  
font_color = "#0A1927"

# The background canvas for your viz
canvas <- grid::rectGrob(
  x = 0, y = 0, 
  width = 16, height = 9,
  gp = grid::gpar(fill = background_color, alpha = 1, col = background_color)
)

# Load in USGS logo (also a black logo available)
usgs_logo <- magick::image_read("../usgs_logo_black.png") 



# Main plot
(main_plot <- SCI_hex_grid |>
    ungroup() %>%
    ggplot() +
    geom_sf(aes(fill = meanSCI),
            color = "black", 
            size = 0.2,
            alpha = 1) +
    scale_fill_scico(palette = "oslo", direction = 1, begin = 0.20, end = 1)+
    theme_void() + 
    theme(legend.position = "none")
)
(past_plot <- SCI_hex_grid |>
    ungroup() %>%
    ggplot() +
    geom_sf(aes(fill = meanPastSCI),
            color = "black", 
            size = 0.2,
            alpha = 1) +
    scale_fill_scico(palette = "oslo", direction = 1, begin = 0.20, end = 1)+
    theme_void() + 
    theme(legend.position = "none")
)
(compare_plot <- SCI_hex_grid |>
  ungroup() %>%
  ggplot() +
  geom_sf(aes(fill = compare_means),
          color = NA, 
          size = 0.1) +
  #scale_fill_gradient2_tableau(palette = "Green-Blue Diverging")+
    scale_fill_scico(palette = "corkO", direction = -1)+
    theme_void() + 
    theme(legend.position = "none"))

(distribution_plot <- ggplot(data = SCI_hex_grid, 
                             aes(y = compare_means,
                                 x = st_coordinates(st_centroid(SCI_hex_grid))[,1]))+
    geom_linerange(aes(ymin = 0, ymax = compare_means, color = compare_means))+
    #geom_linerange(aes(ymin = 0, ymax = compare_means), color = "white")+
    geom_point(aes(color = compare_means))+
    scale_color_scico(palette = "cork", direction = -1)+
    geom_hline(yintercept = 0, color = "#E6EEEC")+
  #scale_color_gradient2_tableau(palette = "Green-Blue Diverging")+
    theme_void()+
    theme(legend.position = "none",
          panel.background = element_blank())
)

```


## Produce final plot

Here, use `cowplot` and `ggsave` to create the final viz for sharing out on Twitter. This template includes adding the USGS logo, title, text, etc.

**Make sure to use the format for saving your png with the date of the prompt that you've been assigned to!** (e.g., `20230401_part-to-whole_cnell.png`)

```{r cowplot, fig.width = 16, fig.height = 9}
ggdraw(ylim = c(0,1), # 0-1 scale makes it easy to place viz items on canvas
       xlim = c(0,1)) +
  # a background
  draw_grob(canvas,
            x = 0, y = 1,
            height = 9, width = 16,
            hjust = 0, vjust = 1) +
  # the main plot
  draw_plot(main_plot,
            x = 0.5,
            y = 0.05,
            width = 0.4,
            hjust = 0.5,
            vjust = 0) +
    draw_plot(past_plot,
            x = 0.5,
            y = 0.35,
            width = 0.2,
            hjust = 0.5) +
  draw_plot(distribution_plot,
            x = 0.5, 
            y = 0,
            width = 0.85,
            height = 0.4,
            hjust = 0.5)+
  # explainer text
  draw_label("Explainer text",
             fontfamily = supporting_font,
             x = 0.96,   
             y = 0.05,
             size = 14,
             hjust = 1,
             vjust = 0,
             color = font_color)+
  # Title
  draw_label("Tale of two winters",
             x = 0.04,
             y = 0.885,
             hjust = 0,
             vjust = 1,
             lineheight = 0.75,
             fontfamily = font_legend,
             color = font_color,
             size = 55) +
    draw_label("For one half of the country,\nit was remarkably snowy;\n   for the other,\n     remarkably not.",
             x = 0.04,
             y = 0.785,
             hjust = 0,
             vjust = 1,
             lineheight = 0.75,
             fontfamily = font_legend,
             color = font_color,
             size = 35) +
  # Add logo
  draw_image(usgs_logo, 
             x = 0.03,
             y = 0.05,
             width = 0.1, 
             hjust = 0, vjust = 0, 
             halign = 0, valign = 0)+
  draw_text("longitude",
            x = 0.92,
            y = 0.18, color = "#386CB1")+
  draw_line(x = c(0.08, 0.08),
            y = c(0.2, 0.30),
            color = "#386CB1",
            )+
  draw_line(x = c(0.075, 0.08, 0.085),
            y = c(0.29, 0.30, 0.29),
            color = "#386CB1")+
  draw_text("Higher than average\nsnow covered area",
            x = 0.08,
            y = 0.3)

# Save the final image in Twitter's 16 by 9 format
# !! Use format for saving with the date of your prompt: 
#         YYYYMMDD_prompt_name ()
# e.g. `20230101_part-to-whole-cnell.png`
ggsave(filename = "out/20230409_high-low_aaarcher.png", 
       width = 16, height = 9, dpi = 300)
```

## Supporting information

### Key takeaways of this viz (1-2 sentences each)

1. Key takeaways here

### Data source(s)

Data sources here.

