---
title: "Example for Chart Challenge"
author: "Althea Archer"
date: "2023-03-10"
output: html_document
---

## Instructions for using this template

> Make sure you have copied the template subdirectory and named it with your prompt's date, prompt's name and your name (e.g., `/01_part-to-whole_cnell`) before editing this document!

1. Put all comments either in the html text (like this) or in the `r` chunks (delineated with 3 "`" marks) to explain your process to Vizlab reviewers
2. Put all R code in the chunks. They're organized for you to (in order): do some set-up steps (load libraries), load in any data (manually or with built in packages), process your data for plotting, create plot using ggplot, and finally compose the twitter image with cowplot.
3. You can run individual code chunks while developing code and plots. Note: the plots that are created in the RStudio coding window *do not* match the plot as created through the ggsave function. 
4. To preview the final plot, use the `Knit` button above. That will create a preview of the final plot in the output `html` and will save the `png` in the `out/` folder. On many systems, the preview in the script document and the html will not match the output `png`, so please refer to the `png` for final proofing.
5. When you're happy with the final plot, fill out the supporting information at the bottom and push to gitlab for review. Note: only commit input data if it can't be downloaded directly with code (e.g., `sbtools` or `dataRetrieval`).

This website has great information on how to use RMarkdown: https://yihui.org/knitr/

## Set up

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```



Define libraries here. 

```{r libraries, warning=FALSE, message=FALSE}
# Load libraries
library(tidyverse) # includes ggplot
library(readr) # best for reading and writing csvs

library(sbtools)
library(lubridate)
library(sf)
library(spData)

# These are used for the layout of the viz
library(cowplot) # for laying out the final plot
library(sysfonts) # for text editing
library(showtext) # for adding in google fonts
library(magick) # for adding logo
```



## Load files

Save any files that you are using directly in the `in` folder. Then read them in to the environment with this chunk of code. Remember, use the RStudio project and relative file pathways, and never include any personal computer file pathways.

This is also a good place to load any data from other built-in packages such as `dataRetrieval` or `spData`, etc.


```{r load}
# Add your code here
# Load drought properties data from ScienceBase
#authenticate_sb() (username includes email address)


# Drought Properties - 7day window - 1951-2020 dataset
drought_properties_7d_file <- "in/Drought_Properties_jd_07d_wndw.csv"
if(!file.exists(drought_properties_7d_file)){ # if files don't exist, download
  sbtools::item_file_download(sb_id = "627974ccd34e8d45aa6e3c81",
                              names = "Drought_Properties_jd_07d_wndw.csv",
                              destinations =  "in/Drought_Properties_jd_07d_wndw.csv",
                              overwrite_file = FALSE)
}
drought_properties_7d <- readr::read_csv(drought_properties_7d_file) 

# Drought Properties - 7day window - 1921-2020 dataset
drought_properties_7d_file100 <- "in/Drought_Properties_jd_07d_wndw_100.csv"
if(!file.exists(drought_properties_7d_file100)){ # if files don't exist, download
  sbtools::item_file_download(sb_id = "62f463c5d34eacf5397395d9",
                              names = "Drought_Properties_jd_07d_wndw.csv",
                              destinations =  "in/Drought_Properties_jd_07d_wndw_100.csv",
                              overwrite_file = FALSE)
}
drought_properties_7d_100yr <- readr::read_csv(drought_properties_7d_file100) 


file_in_metadata <- "in/study_watersheds_metadata.csv"
if(!file.exists(file_in_metadata)){ # if files don't exist, download
  sbtools::item_file_download(sb_id = "62793493d34e8d45aa6e3ba9",
                              names = "study_watersheds_metadata.csv",
                              destinations = file_in_metadata,
                              overwrite_file = F)
}
metadata <- readr::read_csv(file_in_metadata, show_col_types = FALSE) |>
  select(StaID, STANAME, DRAIN_SQKM, HUC02, LAT_GAGE, LNG_GAGE, STATE,
         national_1981, national_1951, national_1921, HCDN_2009)


```

## Get data ready for plotting
  

### Spatial mapping data

```{r spatial}
# Download US State boundaries as sf object
states_sf <- spData::us_states
# add in state names, too
st_crosswalk <- data.frame(NAME = state.name,
                           STATE = state.abb)
# Merge in CASC info
states_CASC <- states_sf |>
  mutate("CASC" = case_when(NAME %in% c("Minnesota", "Iowa", "Missouri", 
                                       "Wisconsin", "Illinois", "Indiana", 
                                       "Michigan", "Ohio") ~ "Midwest",
                          NAME %in% c("Montana", "Wyoming", "Colorado", "North Dakota", 
                                       "South Dakota", "Nebraska", "Kansas") ~ "North Central",
                          NAME %in% c("Maine", "New Hampshire", "Vermont", "Massachusetts", 
                                       "Connecticut", "Rhode Island",
                                       "New York", "New Jersey", "Pennsylvania", 
                                       "Delaware", "Maryland", "West Virginia", 
                                       "Virginia", "Kentucky", "District of Columbia") ~ "Northeast",
                          NAME %in% c("Washington", "Oregon", "Idaho") ~ "Northwest",
                          NAME %in% c("New Mexico", "Texas", 
                                      "Oklahoma", "Louisiana") ~ "South Central",
                          NAME %in% c("North Carolina", "South Carolina", "Georgia", "Alabama", 
                                       "Mississippi", "Florida", "Tennessee", 
                                       "Arkansas") ~ "Southeast",
                          NAME %in% c("Arizona",
                                       "Utah", "Nevada") ~ "Southwest",
                          NAME == "California" ~ "California",
                          TRUE ~ "not sorted")) |>
  left_join(st_crosswalk, by = "NAME")

# convert id to CASC for mapping just regions to make them look "exploded"
mapping_data_regions <- states_CASC |>
  group_by(CASC) |>
  summarise(id = unique(CASC))

# CASC names 
CASC_names <- unique(states_CASC$CASC)
```
  
### Prepping data for when is drought


```{r processing_timing}
# Select appropriate threshold (here 2%)
droughts_2pct <- drought_properties_7d |> 
  # step 0: join metadata and take out DC
  left_join(metadata |> select(StaID, STATE), by = "StaID") |>
  filter(! STATE == "DC") |>
  # step 1: 2% threshold
  filter(threshold == 2) |>
  mutate(year = lubridate::year(start)) |>
  # remove couple from 2020 (uncomplete year)
  filter(year < 2020) 

droughts_100perYear <- droughts_2pct |>
  # step 2: group by year
  group_by(year) |>
  # and get top 100
  slice_max(order_by = severity, n = 100)


droughts_100_join_CASC <- droughts_100perYear |>
  # step 4: join with CASCs
  mutate(CASC = case_when(STATE %in% c("MN", "IA", "MO", "WI", "IL", "IN", "MI", "OH") ~ "Midwest",
                          STATE %in% c("MT", "WY", "CO", "ND", "SD", "NE", "KS") ~ "North Central",
                          STATE %in% c("ME", "NH", "VT", "MA", "CT", "RI", "DC",
                                       "NY", "NJ", "PA", "DE", "MD", "WV", "VA", "KY") ~ "Northeast",
                          STATE %in% c("WA", "OR", "ID") ~ "Northwest",
                          STATE %in% c("NM", "TX", "OK", "LA") ~ "South Central",
                          STATE %in% c("NC", "SC", "GA", "AL", "MS", "FL", "TN", "AR") ~ "Southeast",
                          STATE %in% c("AZ", "UT", "NV") ~ "Southwest",
                          STATE %in% "CA" ~ "California",
                          TRUE ~ "not sorted"),
         north_south = case_when(CASC %in% c("Northwest", "North Central", "Midwest", "Northeast") ~ "North",
                                 TRUE ~ "South"),
         CASC = factor(CASC, levels = c("California", "Southwest", "South Central", "Southeast",
                                        "Northeast", "Midwest", "North Central", "Northwest")))

# Number of 100 most severe droughts by year in each CASC
table(droughts_100_join_CASC$CASC)
table(droughts_100_join_CASC$north_south)
length(table(droughts_100_join_CASC$STATE))

# Make long
droughts_long <- droughts_100_join_CASC |>
  uncount(duration, .id = "day_of_drought") |>
  # for future munging
  mutate(drought_date = start + day_of_drought -1,
         decade = year(floor_date(start, years(10))),
         year = year(drought_date),
         jd = yday(drought_date),
         drought_day_noYr = format(as.Date(drought_date), "%m-%d"),
         drought_day_fakeYr = as.Date(sprintf("1999-%s", drought_day_noYr), "%Y-%m-%d")) |>
  # make sure they're sorted by date
  arrange(start) |>
  # remove unneeded fields
  select(-threshold, -previous_end, -days_since_previous_drought,
         -has_the_potential_to_be_impacted_by_missing_values) 

# Which streamgages? This is used for determining where droughts are
gages_with_drought <- droughts_100_join_CASC |> 
  group_by(StaID) |>
  summarise(num_droughts = n()) |>
  left_join(metadata, by = "StaID")

```

### Processing for the "where is drought" viz

```{r processing_100yr}
# Select appropriate threshold (here 2%)
droughts_2pct_100yr <- drought_properties_7d_100yr |> 
  # step 0: join metadata and take out DC
  left_join(metadata |> select(StaID, STATE), by = "StaID") |>
  filter(! STATE == "DC") |>
  # step 1: 2% threshold
  filter(threshold == 2) |>
  mutate(year = lubridate::year(start)) |>
  # remove couple from 2020 (uncomplete year)
  filter(year < 2020) 

droughts_100perYear_100yr <- droughts_2pct_100yr |>
  # step 2: group by year
  group_by(year) |>
  # and get top 100
  slice_max(order_by = severity, n = 100)

droughts_100_join_CASC_100yr <- droughts_100perYear_100yr |>
  # step 4: join with CASCs
  mutate(CASC = case_when(STATE %in% c("MN", "IA", "MO", "WI", "IL", "IN", "MI", "OH") ~ "Midwest",
                          STATE %in% c("MT", "WY", "CO", "ND", "SD", "NE", "KS") ~ "North Central",
                          STATE %in% c("ME", "NH", "VT", "MA", "CT", "RI", "DC",
                                       "NY", "NJ", "PA", "DE", "MD", "WV", "VA", "KY") ~ "Northeast",
                          STATE %in% c("WA", "OR", "ID") ~ "Northwest",
                          STATE %in% c("NM", "TX", "OK", "LA") ~ "South Central",
                          STATE %in% c("NC", "SC", "GA", "AL", "MS", "FL", "TN", "AR") ~ "Southeast",
                          STATE %in% c("AZ", "UT", "NV") ~ "Southwest",
                          STATE %in% "CA" ~ "California",
                          TRUE ~ "not sorted"),
         CASC = factor(CASC, levels = c("California", "Southwest", "South Central", "Southeast",
                                        "Northeast", "Midwest", "North Central", "Northwest")))

# Which streamgages? This is used for determining where droughts are
gages_with_drought_100yr <- droughts_100_join_CASC_100yr |> 
  group_by(StaID) |>
  summarise(num_droughts = n()) |>
  left_join(metadata, by = "StaID")

gages_with_drought_decade_100yr <- droughts_100_join_CASC_100yr |> 
  mutate(decade = year(floor_date(start, years(10)))) |>
  group_by(StaID, decade) |>
  summarise(num_droughts = n()) |>
  left_join(metadata, by = "StaID")



```
### Making the "when is drought" viz

```{r plot_tests1}
(map_plot <- ggplot(data = states_CASC) +
    geom_sf(fill = "#cdd422", color = "white") +
    geom_sf(data = mapping_data_regions, color = "white", fill = NA, linewidth = 1.5)+
    geom_segment(aes(x = -125, xend = -127, 
                     y = 50, yend = 52), 
                 arrow = arrow(length = unit(0.03, "npc")))+
    geom_segment(aes(x = -104, xend = -104, 
                     y = 50, yend = 52), 
                 arrow = arrow(length = unit(0.03, "npc")))+
    geom_segment(aes(x = -85, xend = -75, 
                     y = 48, yend = 52), 
                 arrow = arrow(length = unit(0.03, "npc")))+
    geom_segment(aes(x = -74, xend = -70, 
                     y = 39, yend = 39), 
                 arrow = arrow(length = unit(0.03, "npc")))+
    geom_segment(aes(x = -124, xend = -127, 
                     y = 37, yend = 37), 
                 arrow = arrow(length = unit(0.03, "npc")))+
    geom_segment(aes(x = -113, xend = -127, 
                     y = 30, yend = 24), 
                 arrow = arrow(length = unit(0.03, "npc")))+
    geom_segment(aes(x = -101, xend = -101, 
                     y = 26, yend = 24), 
                 arrow = arrow(length = unit(0.03, "npc")))+
    geom_segment(aes(x = -80, xend = -70, 
                     y = 30, yend = 24), 
                 arrow = arrow(length = unit(0.03, "npc")))+
   geom_point(data = gages_with_drought,
              aes(y = LAT_GAGE, x = LNG_GAGE, size = I(num_droughts/8)))+
   theme_void() +
    theme(legend.position = "none")
)
```



```{r plot_tests3}


(CASC_season_plot <- ggplot(data = droughts_long, 
                     aes(x = drought_day_fakeYr, group = desc(decade))) +
   geom_histogram(aes(fill = decade), boundary = 0, bins = 36)+
   facet_wrap(~ CASC)+
   scale_x_date(breaks = "month", date_labels = "%b", 
                limits = as.Date(c("1999-01-01", "1999-12-31")),
                expand = c(-0.02,0))+
   coord_polar()+
   theme_void()+
   ylim(c(0, 2500))+
   scale_fill_viridis_c(end = 0.7, direction = -1)+
   theme(legend.position = "none")
 )
```


```{r casc_seasons}
# Chart settings
Ymargin <-  0.04
Xmargin <- 0.04

# CASC Plots
Xbase <- 1-Xmargin # from right
Ybase <- Ymargin*2 # from bottom
Yjitter <- (0.8 - 2*Ymargin)/3
Xjitter <- (1 - 2*Xmargin)/3

# Function to make regional plots
draw_CASC_plots <- map(1:length(CASC_names), function(x){
  # create temporary plot
  plot_temp <- droughts_long |>
    filter(CASC == CASC_names[x]) |> 
    ggplot(aes(x = drought_day_fakeYr, group = desc(decade))) +
    geom_histogram(aes(fill = decade), boundary = 0, bins = 36)+
    scale_x_date(breaks = "month", date_labels = "%b", 
                 limits = as.Date(c("1999-01-01", "1999-12-31")),
                 expand = c(-0.02,0))+
    coord_polar()+
    theme_void()+
    ylim(c(-500, 2500))+
    scale_fill_viridis_c(end = 0.7, direction = -1)+
    theme(legend.position = "none")
  
  if(CASC_names[x] %in% c("Southwest", "Southeast", "South Central")){
    Y_position <- Ybase
  } else if(CASC_names[x] %in% c("California", "Northeast")){
    Y_position <- Ybase + Yjitter
  } else {
    Y_position <- Ybase + 2*Yjitter
  }
  
  if(CASC_names[x] %in% c("Northwest", "California", "Southwest")){
    X_position <- Xbase - (2*Xjitter)
  } else if(CASC_names[x] %in% c("North Central", "South Central")){
    X_position <- Xbase - Xjitter
  } else {
    X_position <- Xbase
  } 
  
  draw_plot(plot_temp,
            x = X_position,
            y = Y_position,
            hjust = 1,
            height = Yjitter,
            width = Xjitter) 

})
```




## Set up main plot

This chunk is where the main ggplot grob definition and set-up occurs. You may have several ggplot grobs, depending on your approach. For each one, if you define it with the `<-` assignment operator and surround the whole code statement in parentheses, you'll be able to preview here what the plot looks like before the next cowplot step.

> Note: The hardest part of this process is getting text to be the optimum size in the final `png` for Twitter. The font sizes here will **not** be the same as in the final image after the cowplot step. Make sure to check the output `png` for true font sizing and plot composition. 

```{r plotting}
# Load some custom fonts and set some custom settings
font_legend <- "Pirata One"
sysfonts::font_add_google("Pirata One")
supporting_font <- "Source Sans Pro"
sysfonts::font_add_google("Source Sans Pro")
showtext::showtext_opts(dpi = 300, regular.wt = 200, bold.wt = 700)
showtext::showtext_auto(enable = TRUE)

# Define colors
background_color = "#c2dde6"
font_color = "#431c5d"

# The background canvas for your viz
canvas <- grid::rectGrob(
  x = 0, y = 0, 
  width = 16, height = 16,
  gp = grid::gpar(fill = background_color, alpha = 1, col = background_color)
)

# Load in USGS logo (also a black logo available)
usgs_logo <- magick::image_read("../usgs_logo_white.png") 




```


## Produce final plot

Here, use `cowplot` and `ggsave` to create the final viz for sharing out on Twitter. This template includes adding the USGS logo, title, text, etc.

**Make sure to use the format for saving your png with the date of the prompt that you've been assigned to!** (e.g., `20230401_part-to-whole_cnell.png`)

```{r cowplot, fig.width = 16, fig.height = 16}
ggdraw(ylim = c(0,1), # 0-1 scale makes it easy to place viz items on canvas
       xlim = c(0,1)) +
  # a background
  draw_grob(canvas,
            x = 0, y = 1,
            height = 16, width = 16,
            hjust = 0, vjust = 1) +
  # the main plot
  draw_CASC_plots +
  draw_plot(map_plot,
            y = 0.44,
            x = 0.5,
            hjust = 0.5,
            vjust = 0.5, 
            width = 0.4) +
  # explainer text
  draw_label("Explainer text",
             fontfamily = supporting_font,
             x = 0.96,   
             y = 0.05,
             size = 14,
             hjust = 1,
             vjust = 0,
             color = font_color)+
  # Title
  draw_label("Title",
             x = 0.04,
             y = 0.285,
             hjust = 0,
             vjust = 1,
             lineheight = 0.75,
             fontfamily = font_legend,
             color = font_color,
             size = 55) +
  # Add logo
  draw_image(usgs_logo, 
             x = 0.04,
             y = 0.05,
             width = 0.1, 
             hjust = 0, vjust = 0, 
             halign = 0, valign = 0)

# Save the final image in Twitter's 16 by 9 format
# !! Use format for saving with the date of your prompt: 
#         YYYYMMDD_prompt_name ()
# e.g. `20230101_part-to-whole-cnell.png`
ggsave(filename = "out/20230423_timeseries-tiles_whenDrought_aaarcher.png", 
       width = 16, height = 16, dpi = 300)
```

## Supporting information

### Key takeaways of this viz (1-2 sentences each)

1. Key takeaways here

### Data source(s)

Data sources here.

